{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urldata = pd.read_pickle('mbti_게시글_url')\n",
    "urllist = urldata['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111079"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = list()\n",
    "mbti_list = list()\n",
    "gender_list = list()\n",
    "datetime_list = list()\n",
    "text_list = list()\n",
    "url_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(82500,len(urllist)):\n",
    "    page = requests.get(urllist[i])\n",
    "    html = page.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        num_page = int(re.findall('[0-9]+', soup.select('span > a')[0].text)[1])\n",
    "    except:\n",
    "        num_page = 1\n",
    "\n",
    "    for j in range(0,num_page):\n",
    "        page = requests.get(urllist[i][:-5]+\"-\"+str(j+1)+\".html\")\n",
    "        print(urllist[i][:-5]+\"-\"+str(j+1)+\".html\")\n",
    "        html = page.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        userinfo = soup.select('div.postdetails > div.userinfo') # 유저 정보\n",
    "        datetime = soup.select('div.posthead > span.postdate.old > span') # 날짜\n",
    "        texts = soup.find_all('blockquote',{'class':'postcontent restore'}) # 본문  --dk만 띄어쓰기 없음('postcontent restore')\n",
    "        \n",
    "        for m in range(len(texts)): # 텍스트 처리 용  for 문\n",
    "            try:\n",
    "                texts[m].find('div',{'class':'bbcode_container'}).decompose()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                texts[m].find('div',{'class':'lockerdome'}).decompose()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # site link 처리 보류\n",
    "#             try:\n",
    "#                 texts[m].find('a').decompose()\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "        \n",
    "        for k in range(len(userinfo)):\n",
    "            try:\n",
    "                name_list.append(userinfo[k].find('a',{'rel':'nofollow'}).text) \n",
    "            except:\n",
    "                name_list.append(\"0\")\n",
    "\n",
    "            try:\n",
    "                gender_list.append(userinfo[k].find('img')['src'].split('/')[-1].split('.')[0])\n",
    "            except:\n",
    "                gender_list.append(\"0\")\n",
    "            \n",
    "            try:\n",
    "                userinfo[k].find('a',{'rel':'nofollow'}).decompose() \n",
    "                mbti_list.append(userinfo[k].text)\n",
    "            except:\n",
    "                mbti_list.append(\"0\")\n",
    "            \n",
    "            try:\n",
    "                text_list.append(texts[k].get_text(strip=True))\n",
    "            except:\n",
    "                text_list.append(\"0\")\n",
    "\n",
    "            try:\n",
    "                datetime_list.append(datetime[k].text)\n",
    "            except:\n",
    "                datetime_list.append(\"0\")\n",
    "                \n",
    "            try:\n",
    "                url_list.append(urllist[i][:-5]+\"-\"+str(j+1)+\".html\")\n",
    "            except:\n",
    "                url_list.append(\"0\")\n",
    "            \n",
    "        print(j+1,\"/\",num_page)\n",
    "    print(len(name_list),len(mbti_list),len(gender_list),len(text_list),len(datetime_list),len(url_list))\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        mbti_ls0 = [name_list,mbti_list,gender_list,text_list,datetime_list,url_list]\n",
    "        mbti_ls = list(map(list,zip(*mbti_ls0)))\n",
    "        mbti_df = pd.DataFrame(mbti_ls,columns =['name_list','mbti_list','gender_list','text_list','datetime_list','url_list'])\n",
    "\n",
    "        mbti_df.to_csv('./mbti_df/mbti_data'+str(i)+'.csv',index=False)   \n",
    "        \n",
    "        name_list = list()\n",
    "        mbti_list = list()\n",
    "        gender_list = list()\n",
    "        datetime_list = list()\n",
    "        text_list = list()\n",
    "        url_list = list()\n",
    "\n",
    "mbti_ls0 = [name_list,mbti_list,gender_list,text_list,datetime_list,url_list]\n",
    "mbti_ls = list(map(list,zip(*mbti_ls0)))\n",
    "mbti_df = pd.DataFrame(mbti_ls,columns =['name_list','mbti_list','gender_list','text_list','datetime_list','url_list'])\n",
    "\n",
    "mbti_df.to_csv('./mbti_df/mbti_data'+str(i)+'.csv',index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 csv 리스트\n",
    "import os\n",
    "csv_list = os.listdir(\"./mbti_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n"
     ]
    }
   ],
   "source": [
    "# csv concat\n",
    "dff = pd.read_csv(\"./mbti_df/\" + csv_list[0])\n",
    "for i, c in enumerate(csv_list[1:]):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    df = pd.read_csv(\"./mbti_df/\" + c)\n",
    "    dff = pd.concat([dff, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604589"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 사용자 수\n",
    "sum(dff['name_list'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619499, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 저장\n",
    "dff.to_csv(\"mbti_df_82500-111079.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
